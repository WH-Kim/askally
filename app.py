# app.py

import os
import streamlit as st
import pandas as pd
import asyncio
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama
from langgraph.graph import END

from modules.graph import create_supervisor_graph
from modules.utils import get_db_schema_and_samples, load_few_shot_examples_from_jsonl, load_or_create_vector_db, get_indexed_doc_samples, save_uploaded_files
from modules.config import DB_PATH, AVAILABLE_OPENAI_MODELS, AVAILABLE_OLLAMA_MODELS, RAG_DOCUMENTS_PATH, VECTOR_DB_PATH

RECURSION_LIMIT = 15

# --- Streamlit UI êµ¬ì„± ---
st.set_page_config(page_title="ğŸ¤– Dynamic Supervisor Agent", page_icon="ğŸ¤–", layout="wide")

# --- ì‚¬ì´ë“œë°” ---
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì •")
    
    # 1. LLM ëª¨ë¸ ì„ íƒ (í†µí•© ë©”ë‰´)
    model_options = [f"OpenAI: {m}" for m in AVAILABLE_OPENAI_MODELS] + [f"Ollama: {m}" for m in AVAILABLE_OLLAMA_MODELS]
    
    if 'selected_model_option' not in st.session_state:
        st.session_state.selected_model_option = model_options[0]
        
    selected_option = st.selectbox("LLM ëª¨ë¸ ì„ íƒ", options=model_options, key="selected_model_option")
    
    provider, model_name = selected_option.split(": ")
    st.session_state.model_provider = provider
    st.session_state.selected_model = model_name
    
    st.markdown("---")
    if st.button("ëŒ€í™” ì´ˆê¸°í™” ğŸ”„", use_container_width=True, type="primary"):
        st.session_state.clear()
        st.rerun()

# --- ë©”ì¸ í™”ë©´ ---
st.title("ğŸ¤– Dynamic Supervisor Multi-Agent Chat")
st.markdown(f"##### í˜„ì¬ ëª¨ë¸: `{st.session_state.get('model_provider', 'OpenAI')}: {st.session_state.get('selected_model', AVAILABLE_OPENAI_MODELS[0])}`")

cols = st.columns(3)
with cols[0]:
    if st.button("ğŸ’¬ ì¼ë°˜ ëŒ€í™”", use_container_width=True):
        st.session_state.chat_mode = "DirectResponse"
        st.toast("ì¼ë°˜ ëŒ€í™” ëª¨ë“œê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
with cols[1]:
    if st.button("ğŸ“„ RAG ë¬¸ì„œì— ì§ˆë¬¸í•˜ê¸°", use_container_width=True):
        st.session_state.chat_mode = "PDF_RAG_Agent"
        st.toast("RAG ì§ˆë¬¸ ëª¨ë“œê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
with cols[2]:
    if st.button("ğŸ—ƒï¸ ë°ì´í„°ë² ì´ìŠ¤ì— ì§ˆë¬¸í•˜ê¸°", use_container_width=True):
        st.session_state.chat_mode = "SQLAgent"
        st.toast("ë°ì´í„°ë² ì´ìŠ¤ ì§ˆë¬¸ ëª¨ë“œê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")

# --- ì‘ì—… ëª¨ë“œë³„ ì •ë³´ í‘œì‹œ ---
if st.session_state.get("chat_mode") == "PDF_RAG_Agent":
    with st.expander("ğŸ“„ RAG ë¬¸ì„œ ì •ë³´ ë° ê´€ë¦¬", expanded=False):
        if 'vector_store' not in st.session_state:
            st.session_state.vector_store = load_or_create_vector_db(RAG_DOCUMENTS_PATH, VECTOR_DB_PATH)
        
        st.subheader("ì¸ë±ì‹±ëœ ë¬¸ì„œ ëª©ë¡")
        indexed_docs = get_indexed_doc_samples(st.session_state.vector_store)
        if indexed_docs:
            for doc in indexed_docs:
                st.caption(f"- {doc}")
        else:
            st.info(f"`{RAG_DOCUMENTS_PATH}` í´ë”ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

        st.subheader("ìƒˆ ë¬¸ì„œ ì¶”ê°€")
        uploaded_files = st.file_uploader("ì—…ë¡œë“œí•  PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.", type="pdf", accept_multiple_files=True)
        if st.button("ì„ íƒí•œ íŒŒì¼ ì—…ë¡œë“œ ë° ì¬ì¸ë±ì‹±", use_container_width=True):
            if uploaded_files:
                save_uploaded_files(uploaded_files, RAG_DOCUMENTS_PATH)
                # ë²¡í„° DBë¥¼ ê°•ì œë¡œ ì¬ìƒì„±í•˜ë„ë¡ ì„¸ì…˜ ìƒíƒœë¥¼ ì‚­ì œ
                if 'vector_store' in st.session_state:
                    del st.session_state.vector_store
                st.success("íŒŒì¼ ì—…ë¡œë“œ ë° ì¬ì¸ë±ì‹± ì™„ë£Œ! í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ì ìš©í•˜ì„¸ìš”.")
                st.rerun()
            else:
                st.warning("ë¨¼ì € íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

if st.session_state.get("chat_mode") == "SQLAgent":
    with st.expander("ğŸ—‚ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ í™•ì¸í•˜ê¸°", expanded=False):
        tab1, tab2 = st.tabs(["ë°ì´í„° ìƒ˜í”Œ", "Few-Shot ì˜ˆì‹œ"])
        
        with tab1:
            st.subheader("í…Œì´ë¸” ìƒ˜í”Œ ë°ì´í„°")
            db_samples = get_db_schema_and_samples(DB_PATH)
            if db_samples:
                for table, df in db_samples.items():
                    st.write(f"**- í…Œì´ë¸”: `{table}`**")
                    st.dataframe(df, use_container_width=True, height=150)
            else:
                st.warning("ë°ì´í„°ë² ì´ìŠ¤ ìƒ˜í”Œì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        with tab2:
            st.subheader("ì§ˆì˜-ì¿¼ë¦¬ Few-Shot ì˜ˆì‹œ (ìë™ ë¡œë“œ)")
            if 'few_shot_examples' not in st.session_state:
                st.session_state.few_shot_examples = load_few_shot_examples_from_jsonl("few_shot_examples.jsonl")
            
            few_shot_examples = st.session_state.get("few_shot_examples", [])
            if few_shot_examples:
                df_examples = pd.DataFrame(few_shot_examples)
                st.dataframe(df_examples, use_container_width=True)
            else:
                st.warning("`few_shot_examples.jsonl` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

st.markdown("<hr>", unsafe_allow_html=True)
st.markdown("""<style>
    .stChatMessage { border-radius: 10px; padding: 10px; margin-bottom: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); }
    .stChatMessage[data-testid="stChatMessage-user"] { background-color: #e1f5fe; }
    .stChatMessage[data-testid="stChatMessage-assistant"] { background-color: #f1f8e9; }
</style>""", unsafe_allow_html=True)

# --- ê·¸ë˜í”„ ë° ì„¸ì…˜ ì´ˆê¸°í™” ---
provider = st.session_state.get("model_provider", "OpenAI")
model_name = st.session_state.get("selected_model", AVAILABLE_OPENAI_MODELS[0])

if provider == "Ollama":
    llm = ChatOllama(model=model_name, temperature=0)
else:
    llm = ChatOpenAI(model=model_name, temperature=0, max_retries=3, streaming=True)

graph = create_supervisor_graph(llm)

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ìœ„ì—ì„œ ëŒ€í™” ëª¨ë“œë¥¼ ì„ íƒí•˜ê³  ì§ˆë¬¸í•´ì£¼ì„¸ìš”."}]
if "thread_id" not in st.session_state:
    st.session_state.thread_id = f"streamlit-thread-{os.urandom(4).hex()}"

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

if prompt := st.chat_input("ì‘ì—… ëª¨ë“œë¥¼ ì„ íƒí•˜ê³  ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    with st.chat_message("assistant"):
        final_response = None
        
        # ì—ì´ì „íŠ¸ ì‘ì—… ê³¼ì •ê³¼ ìµœì¢… ë‹µë³€ì„ ë¶„ë¦¬í•˜ì—¬ í‘œì‹œ
        status_container = st.expander("ì—ì´ì „íŠ¸ ì‘ì—… ê³¼ì •", expanded=True)
        response_container = st.empty()
        
        config = RunnableConfig(recursion_limit=RECURSION_LIMIT, configurable={"thread_id": st.session_state.thread_id})
        
        chat_mode = st.session_state.get("chat_mode", None)
        if not chat_mode:
            final_response = "ë¨¼ì € ìƒë‹¨ì˜ ëŒ€í™” ëª¨ë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”."
        else:
            current_prompt = prompt
            if chat_mode == "PDF_RAG_Agent":
                current_prompt = f"í´ë”ì— ìˆëŠ” PDF ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”: {prompt}"
            elif chat_mode == "SQLAgent":
                current_prompt = f"ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ì¡°íšŒí•˜ì—¬ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”: {prompt}"

            inputs = {"messages": st.session_state.messages} # ì „ì²´ ëŒ€í™” ê¸°ë¡ ì „ë‹¬
            
            async def stream_and_display():
                full_response = ""
                status_updates = []
                
                async for event in graph.astream_events(inputs, config=config, version="v1"):
                    kind = event["event"]
                    name = event["name"]
                    
                    if kind == "on_chain_end":
                        if name == "Supervisor":
                            if event["data"]["output"]:
                                status_updates.append(f"**Supervisor:** ë‹¤ìŒ ì‘ì—…ìë¡œ `{event['data']['output'].next}`ë¥¼ ì„ íƒí–ˆìŠµë‹ˆë‹¤.")
                        elif name in ["SQLAgent", "PDF_RAG_Agent", "DirectResponse"]:
                             status_updates.append(f"**{name}:** ì‘ì—…ì„ ì‹œì‘í•©ë‹ˆë‹¤...")
                    
                    elif kind == "on_tool_start":
                        status_updates.append(f"**{name} (ë„êµ¬ ì‚¬ìš© ğŸ› ï¸):** `{event['name']}` ì‹¤í–‰ ì¤‘...")
                        if event['name'] == 'sql_db_query':
                            status_updates.append(f"```sql\n{event['data']['input']['query']}\n```")
                    
                    elif kind == "on_tool_end":
                        status_updates.append(f"**{name} (ê²°ê³¼ ğŸ“):** `{event['name']}` ì‹¤í–‰ ì™„ë£Œ.")
                    
                    elif kind == "on_chat_model_stream":
                        chunk = event["data"]["chunk"]
                        if isinstance(chunk, AIMessage) and chunk.content:
                            full_response += chunk.content
                            response_container.markdown(full_response + "â–Œ")
                
                return full_response, status_updates

            try:
                # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
                final_response, status_history = asyncio.run(stream_and_display())
                
                # ìµœì¢… ìƒíƒœ ì—…ë°ì´íŠ¸
                with status_container:
                    for update in status_history:
                        st.markdown(update)

            except Exception as e:
                if "recursion limit" in str(e).lower():
                    final_response = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì—ì´ì „íŠ¸ê°€ {RECURSION_LIMIT}ë²ˆì˜ ì‹œë„ í›„ì—ë„ ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
                else:
                    final_response = f"ì£„ì†¡í•©ë‹ˆë‹¤, ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                st.error(final_response)

        if final_response:
            response_container.markdown(final_response)
            st.session_state.messages.append({"role": "assistant", "content": final_response})
        else:
            fallback_message = "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
            st.error(fallback_message)
            if st.session_state.messages and st.session_state.messages[-1]['role'] == 'user':
                st.session_state.messages.pop()