# app.py

import os
import streamlit as st
import pandas as pd
import json
import re
from langchain_core.messages import AIMessage, HumanMessage, AIMessageChunk
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama

from modules.graph import create_supervisor_graph
from modules.utils import get_db_schema_and_samples, load_few_shot_examples_from_jsonl, load_or_create_vector_db, get_indexed_doc_samples, save_uploaded_files
from modules.config import DB_PATH, AVAILABLE_OPENAI_MODELS, AVAILABLE_OLLAMA_MODELS, RAG_DOCUMENTS_PATH, VECTOR_DB_PATH

RECURSION_LIMIT = 50

st.set_page_config(page_title="ğŸ¤– Dynamic Supervisor Agent", page_icon="ğŸ¤–", layout="wide")

# --- Sidebar ---
with st.sidebar:
    st.title("âš™ï¸ ì„¤ì •")
    model_options = [f"OpenAI: {m}" for m in AVAILABLE_OPENAI_MODELS] + [f"Ollama: {m}" for m in AVAILABLE_OLLAMA_MODELS]
    if 'selected_model_option' not in st.session_state:
        st.session_state.selected_model_option = model_options[0]
    selected_option = st.selectbox("LLM ëª¨ë¸ ì„ íƒ", options=model_options, key="selected_model_option")
    provider, model_name = selected_option.split(": ")
    st.session_state.model_provider = provider
    st.session_state.selected_model = model_name
    st.markdown("---")
    if st.button("ëŒ€í™” ì´ˆê¸°í™” ğŸ”„", use_container_width=True, type="primary"):
        st.session_state.clear()
        st.rerun()

# --- Main Page ---
st.title("ğŸ¤– Dynamic Supervisor Multi-Agent Chat")
st.markdown(f"##### í˜„ì¬ ëª¨ë¸: `{st.session_state.get('model_provider', 'OpenAI')}: {st.session_state.get('selected_model', AVAILABLE_OPENAI_MODELS[0])}`")

st.markdown("##### ğŸ’¬ ëŒ€í™” ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”")
cols = st.columns(6)
with cols[0]:
    if st.button("ğŸ¤– ìë™ (Supervisor)", use_container_width=True): st.session_state.chat_mode = "Supervisor"
with cols[1]:
    if st.button("ğŸ“„ RAG", use_container_width=True): st.session_state.chat_mode = "RAGAgent"
with cols[2]:
    if st.button("ğŸ—ƒï¸ DB ì§ˆë¬¸", use_container_width=True): st.session_state.chat_mode = "SQLAgent"
with cols[3]:
    if st.button("ğŸ“Š ì°¨íŠ¸ ìƒì„±", use_container_width=True): st.session_state.chat_mode = "ChartGeneratorAgent"
with cols[4]:
    if st.button("ğŸ“„ PDF ë³´ê³ ì„œ", use_container_width=True): st.session_state.chat_mode = "ReportAgent"
with cols[5]:
    if st.button("ğŸ’¬ ì¼ë°˜ ëŒ€í™”", use_container_width=True): st.session_state.chat_mode = "DirectResponse"


if "chat_mode" not in st.session_state:
    st.session_state.chat_mode = "Supervisor"
st.info(f"í˜„ì¬ ëª¨ë“œ: **{st.session_state.chat_mode}**")


# --- ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„ ---
@st.cache_resource
def load_db_info():
    db_samples = get_db_schema_and_samples(DB_PATH)
    if not db_samples: return None
    # few_shot_examples.jsonl íŒŒì¼ì´ ì—†ì–´ë„ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì§€ ì•Šë„ë¡ ì²˜ë¦¬
    try:
        few_shot_examples = load_few_shot_examples_from_jsonl("few_shot_examples.jsonl")
    except FileNotFoundError:
        few_shot_examples = []
    return {"few_shot_examples": few_shot_examples}

db_info = load_db_info()

if 'vector_store' not in st.session_state:
    st.session_state.vector_store = load_or_create_vector_db(RAG_DOCUMENTS_PATH, VECTOR_DB_PATH)
retriever = st.session_state.vector_store.as_retriever() if st.session_state.vector_store else None


with st.expander("ğŸ“„ RAG ë¬¸ì„œ ì •ë³´ ë° ê´€ë¦¬", expanded=False):
    st.subheader("ì¸ë±ì‹±ëœ ë¬¸ì„œ ëª©ë¡")
    if st.session_state.vector_store:
        indexed_docs = get_indexed_doc_samples(st.session_state.vector_store)
        if indexed_docs:
            for doc in indexed_docs: st.caption(f"- {doc}")
        else:
            st.info("ì¸ë±ì‹±ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        st.info(f"`{RAG_DOCUMENTS_PATH}` í´ë”ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")

    st.subheader("ìƒˆ ë¬¸ì„œ ì¶”ê°€")
    uploaded_files = st.file_uploader("ì—…ë¡œë“œí•  PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.", type="pdf", accept_multiple_files=True)
    if st.button("ì„ íƒí•œ íŒŒì¼ ì—…ë¡œë“œ ë° ì¬ì¸ë±ì‹±", use_container_width=True):
        if uploaded_files:
            save_uploaded_files(uploaded_files, RAG_DOCUMENTS_PATH)
            if 'vector_store' in st.session_state:
                del st.session_state.vector_store
            st.rerun()
        else:
            st.warning("ë¨¼ì € íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

with st.expander("ğŸ—‚ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ í™•ì¸í•˜ê¸°", expanded=False):
    db_samples = get_db_schema_and_samples(DB_PATH)
    if db_samples:
        for table, df in db_samples.items():
            st.write(f"**- í…Œì´ë¸”: `{table}`**")
            st.dataframe(df, use_container_width=True, height=150)

st.markdown("<hr>", unsafe_allow_html=True)
st.markdown("""<style>
    .stChatMessage { border-radius: 10px; padding: 10px; margin-bottom: 10px; }
    .stChatMessage[data-testid="stChatMessage-user"] { background-color: #e1f5fe; }
    .stChatMessage[data-testid="stChatMessage-assistant"] { background-color: #f1f8e9; }
</style>""", unsafe_allow_html=True)


# --- LLM ë° ê·¸ë˜í”„ ì´ˆê¸°í™” ---
provider = st.session_state.get("model_provider", "OpenAI")
model_name = st.session_state.get("selected_model", AVAILABLE_OPENAI_MODELS[0])
if provider == "Ollama":
    llm = ChatOllama(model=model_name, temperature=0)
else:
    llm = ChatOpenAI(model=model_name, temperature=0, max_retries=3, streaming=True)

graph = create_supervisor_graph(llm, retriever, db_info)


if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ìœ„ì—ì„œ ëŒ€í™” ëª¨ë“œë¥¼ ì„ íƒí•˜ê³  ì§ˆë¬¸í•´ì£¼ì„¸ìš”."}]
if "thread_id" not in st.session_state:
    st.session_state.thread_id = f"streamlit-thread-{os.urandom(4).hex()}"

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        content = msg.get("content", "")
        image_paths = re.findall(r'[\w-]+\.png', content)
        report_paths = re.findall(r'[\w-]+\.pdf', content)
        
        text_content = content
        for path in image_paths + report_paths:
            text_content = text_content.replace(path, "").strip()

        if text_content:
            st.write(text_content)
        
        for path in image_paths:
            if os.path.exists(path):
                st.image(path)
        
        for path in report_paths:
            if os.path.exists(path):
                with open(path, "rb") as f:
                    st.download_button(
                        label=f"ğŸ“„ {os.path.basename(path)} ë‹¤ìš´ë¡œë“œ",
                        data=f,
                        file_name=os.path.basename(path),
                        mime="application/pdf"
                    )

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    with st.chat_message("assistant"):
        with st.spinner("ì—ì´ì „íŠ¸ê°€ ìƒê° ì¤‘ì…ë‹ˆë‹¤..."):
            response_container = st.empty()
            
            config = RunnableConfig(recursion_limit=RECURSION_LIMIT, configurable={"thread_id": st.session_state.thread_id})
            
            # ëŒ€í™” ê¸°ë¡ì„ LangChain ë©”ì‹œì§€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            conversation_history = [
                HumanMessage(content=msg["content"]) if msg["role"] == "user" 
                else AIMessage(content=msg["content"]) 
                for msg in st.session_state.messages
            ]

            chat_mode = st.session_state.get("chat_mode")
            inputs = {"messages": conversation_history}
            if chat_mode != "Supervisor":
                inputs["next"] = chat_mode
            
            full_response = ""
            node_statuses = {}
            active_node_name = None
            final_state = None

            try:
                for chunk in graph.stream(inputs, config=config, stream_mode="updates"):
                    node_name = list(chunk.keys())[0]

                    if node_name == "__end__":
                        final_state = chunk[node_name]
                        break

                    if active_node_name and active_node_name in node_statuses:
                        node_statuses[active_node_name].update(state="complete", expanded=False)

                    active_node_name = node_name
                    node_update = chunk[node_name]
                    
                    if node_name not in node_statuses:
                        node_statuses[node_name] = st.status(f"**ì‹¤í–‰ ì¤‘:** `{node_name}`", state="running", expanded=True)

                    with node_statuses[node_name]:
                        if node_name == "Supervisor":
                            if next_agent := node_update.get("next"): st.markdown(f"â†ªï¸ ë‹¤ìŒ ì‘ì—…ìœ¼ë¡œ **`{next_agent}`** í˜¸ì¶œ")
                        elif node_name == "select_tables": st.markdown(f"**- ê´€ë ¨ëœ í…Œì´ë¸” ì„ íƒ:** `{node_update.get('table_names')}`")
                        elif node_name == "get_schema": st.markdown(f"**- í…Œì´ë¸” ìŠ¤í‚¤ë§ˆ ì¡°íšŒ**")
                        elif node_name == "query_gen": 
                            st.markdown(f"**- SQL ì¿¼ë¦¬ ìƒì„± ì¤‘...**")
                            # [ìˆ˜ì •] query_gen ë…¸ë“œì—ì„œ ìƒì„±ëœ SQL ì¿¼ë¦¬ë¥¼ í™”ë©´ì— í‘œì‹œ
                            if query := node_update.get('query'):
                                 st.markdown(f"**- ìƒì„±ëœ SQL ì¿¼ë¦¬:**")
                                 st.code(query, language='sql')
                        
                        if "result" in node_update:
                            st.markdown("**- ì‹¤í–‰ ê²°ê³¼:**")
                            try:
                                result_data = json.loads(node_update["result"])
                                if "data" in result_data:
                                    df = pd.DataFrame(result_data.get("data", []), columns=result_data.get("columns", []))
                                    st.dataframe(df)
                                elif "error" in result_data: st.error(result_data['error'])
                            except (json.JSONDecodeError, TypeError): st.text(node_update["result"])

                    if "messages" in node_update:
                        last_message = node_update["messages"][-1]
                        if isinstance(last_message, AIMessageChunk):
                            full_response += last_message.content
                            response_container.markdown(full_response + "â–Œ")

                final_answer = ""
                # ìµœì¢… ë‹µë³€ì€ ë§ˆì§€ë§‰ __end__ ìƒíƒœì˜ ë©”ì‹œì§€ì—ì„œ ê°€ì ¸ì˜´
                if final_state and final_state.get("messages"):
                    final_answer = final_state.get("messages", [])[-1].content
                elif full_response:
                    final_answer = full_response

                if final_answer:
                    response_container.markdown(final_answer)
                    st.session_state.messages.append({"role": "assistant", "content": final_answer})
                
                # PDF ìƒì„±ì´ ì™„ë£Œë˜ë©´ reruní•˜ì—¬ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ì„ ì¦‰ì‹œ í‘œì‹œ
                if final_answer and re.search(r'[\w-]+\.pdf', final_answer):
                    st.rerun()

            except Exception as e:
                error_message = f"ì£„ì†¡í•©ë‹ˆë‹¤, ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                st.error(error_message)
                st.session_state.messages.append({"role": "assistant", "content": error_message})