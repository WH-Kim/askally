# app.py

import os
import streamlit as st
import pandas as pd
import asyncio
import json
from langchain_core.messages import AIMessage, HumanMessage
from langchain_core.runnables import RunnableConfig
from langchain_openai import ChatOpenAI
from langchain_community.chat_models import ChatOllama

from modules.graph import create_supervisor_graph
from modules.utils import get_db_schema_and_samples, load_few_shot_examples_from_jsonl, load_or_create_vector_db, get_indexed_doc_samples, save_uploaded_files
from modules.config import DB_PATH, AVAILABLE_OPENAI_MODELS, AVAILABLE_OLLAMA_MODELS, RAG_DOCUMENTS_PATH, VECTOR_DB_PATH

RECURSION_LIMIT = 25

st.set_page_config(page_title="ğŸ¤– Dynamic Supervisor Agent", page_icon="ğŸ¤–", layout="wide")

with st.sidebar:
    st.title("âš™ï¸ ì„¤ì •")
    model_options = [f"OpenAI: {m}" for m in AVAILABLE_OPENAI_MODELS] + [f"Ollama: {m}" for m in AVAILABLE_OLLAMA_MODELS]
    if 'selected_model_option' not in st.session_state:
        st.session_state.selected_model_option = model_options[0]
    selected_option = st.selectbox("LLM ëª¨ë¸ ì„ íƒ", options=model_options, key="selected_model_option")
    provider, model_name = selected_option.split(": ")
    st.session_state.model_provider = provider
    st.session_state.selected_model = model_name
    st.markdown("---")
    if st.button("ëŒ€í™” ì´ˆê¸°í™” ğŸ”„", use_container_width=True, type="primary"):
        st.session_state.clear()
        st.rerun()

st.title("ğŸ¤– Dynamic Supervisor Multi-Agent Chat")
st.markdown(f"##### í˜„ì¬ ëª¨ë¸: `{st.session_state.get('model_provider', 'OpenAI')}: {st.session_state.get('selected_model', AVAILABLE_OPENAI_MODELS[0])}`")

st.markdown("##### ğŸ’¬ ëŒ€í™” ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”")
cols = st.columns(4)
with cols[0]:
    if st.button("ğŸ¤– ìë™ (Supervisor)", use_container_width=True):
        st.session_state.chat_mode = "Supervisor"
        st.toast("ìë™ ëª¨ë“œê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
with cols[1]:
    if st.button("ğŸ“„ RAG ë¬¸ì„œ ì§ˆë¬¸", use_container_width=True):
        st.session_state.chat_mode = "RAGAgent"
        st.toast("RAG ì§ˆë¬¸ ëª¨ë“œê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
with cols[2]:
    if st.button("ğŸ—ƒï¸ DB ì§ˆë¬¸", use_container_width=True):
        st.session_state.chat_mode = "SQLAgent"
        st.toast("ë°ì´í„°ë² ì´ìŠ¤ ì§ˆë¬¸ ëª¨ë“œê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")
with cols[3]:
    if st.button("ğŸ’¬ ì¼ë°˜ ëŒ€í™”", use_container_width=True):
        st.session_state.chat_mode = "DirectResponse"
        st.toast("ì¼ë°˜ ëŒ€í™” ëª¨ë“œê°€ ì„ íƒë˜ì—ˆìŠµë‹ˆë‹¤.")

if "chat_mode" not in st.session_state:
    st.session_state.chat_mode = "Supervisor"
st.info(f"í˜„ì¬ ëª¨ë“œ: **{st.session_state.chat_mode}**")

with st.expander("ğŸ“„ RAG ë¬¸ì„œ ì •ë³´ ë° ê´€ë¦¬", expanded=False):
    if 'vector_store' not in st.session_state:
        st.session_state.vector_store = load_or_create_vector_db(RAG_DOCUMENTS_PATH, VECTOR_DB_PATH)
    st.subheader("ì¸ë±ì‹±ëœ ë¬¸ì„œ ëª©ë¡")
    indexed_docs = get_indexed_doc_samples(st.session_state.vector_store)
    if indexed_docs:
        for doc in indexed_docs:
            st.caption(f"- {doc}")
    else:
        st.info(f"`{RAG_DOCUMENTS_PATH}` í´ë”ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    st.subheader("ìƒˆ ë¬¸ì„œ ì¶”ê°€")
    uploaded_files = st.file_uploader("ì—…ë¡œë“œí•  PDF íŒŒì¼ì„ ì„ íƒí•˜ì„¸ìš”.", type="pdf", accept_multiple_files=True)
    if st.button("ì„ íƒí•œ íŒŒì¼ ì—…ë¡œë“œ ë° ì¬ì¸ë±ì‹±", use_container_width=True):
        if uploaded_files:
            save_uploaded_files(uploaded_files, RAG_DOCUMENTS_PATH)
            if 'vector_store' in st.session_state: del st.session_state.vector_store
            st.success("íŒŒì¼ ì—…ë¡œë“œ ë° ì¬ì¸ë±ì‹± ì™„ë£Œ! í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ì ìš©í•˜ì„¸ìš”.")
            st.rerun()
        else:
            st.warning("ë¨¼ì € íŒŒì¼ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")

with st.expander("ğŸ—‚ï¸ ë°ì´í„°ë² ì´ìŠ¤ ì •ë³´ í™•ì¸í•˜ê¸°", expanded=False):
    tab1, tab2 = st.tabs(["ë°ì´í„° ìƒ˜í”Œ", "Few-Shot ì˜ˆì‹œ"])
    with tab1:
        st.subheader("í…Œì´ë¸” ìƒ˜í”Œ ë°ì´í„°")
        db_samples = get_db_schema_and_samples(DB_PATH)
        if db_samples:
            for table, df in db_samples.items():
                st.write(f"**- í…Œì´ë¸”: `{table}`**")
                st.dataframe(df, use_container_width=True, height=150)
        else:
            st.warning("ë°ì´í„°ë² ì´ìŠ¤ ìƒ˜í”Œì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    with tab2:
        st.subheader("ì§ˆì˜-ì¿¼ë¦¬ Few-Shot ì˜ˆì‹œ (ìë™ ë¡œë“œ)")
        if 'few_shot_examples' not in st.session_state:
            st.session_state.few_shot_examples = load_few_shot_examples_from_jsonl("few_shot_examples.jsonl")
        few_shot_examples = st.session_state.get("few_shot_examples", [])
        if few_shot_examples:
            df_examples = pd.DataFrame(few_shot_examples)
            st.dataframe(df_examples, use_container_width=True)
        else:
            st.warning("`few_shot_examples.jsonl` íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

st.markdown("<hr>", unsafe_allow_html=True)
st.markdown("""<style>
    .stChatMessage { border-radius: 10px; padding: 10px; margin-bottom: 10px; box-shadow: 0 2px 4px rgba(0,0,0,0.05); }
    .stChatMessage[data-testid="stChatMessage-user"] { background-color: #e1f5fe; }
    .stChatMessage[data-testid="stChatMessage-assistant"] { background-color: #f1f8e9; }
</style>""", unsafe_allow_html=True)

provider = st.session_state.get("model_provider", "OpenAI")
model_name = st.session_state.get("selected_model", AVAILABLE_OPENAI_MODELS[0])

if provider == "Ollama":
    llm = ChatOllama(model=model_name, temperature=0)
else:
    llm = ChatOpenAI(model=model_name, temperature=0, max_retries=3, streaming=True)

graph = create_supervisor_graph(llm)

if "messages" not in st.session_state:
    st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ìœ„ì—ì„œ ëŒ€í™” ëª¨ë“œë¥¼ ì„ íƒí•˜ê³  ì§ˆë¬¸í•´ì£¼ì„¸ìš”."}]
if "thread_id" not in st.session_state:
    st.session_state.thread_id = f"streamlit-thread-{os.urandom(4).hex()}"

for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

if prompt := st.chat_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    with st.chat_message("assistant"):
        final_response = None
        status_container = st.expander("ì—ì´ì „íŠ¸ ì‘ì—… ê³¼ì •", expanded=True)
        response_container = st.empty()
        config = RunnableConfig(recursion_limit=RECURSION_LIMIT, configurable={"thread_id": st.session_state.thread_id})
        
        conversation_history = [HumanMessage(content=msg["content"]) if msg["role"] == "user" else AIMessage(content=msg["content"]) for msg in st.session_state.messages]
        chat_mode = st.session_state.get("chat_mode")
        inputs = {"messages": conversation_history}
        
        if chat_mode != "Supervisor":
            inputs["next"] = chat_mode
        
        async def stream_and_display():
            final_answer = None
            status_placeholder = status_container.empty()
            status_updates = []
            
            final_state = await asyncio.to_thread(graph.invoke, inputs, config=config)

            final_messages = final_state.get("messages", [])
            if final_messages:
                final_answer = final_messages[-1].content
            
            with status_container:
                st.markdown("#### ì—ì´ì „íŠ¸ ì‘ì—… ìš”ì•½")
                if "query" in final_state and final_state["query"]:
                     st.markdown(f"**Generated SQL Query ğŸ”**\n```sql\n{final_state['query']}\n```")
                
                if "result" in final_state and final_state["result"]:
                    try:
                        result_data = json.loads(final_state["result"])
                        if "error" in result_data:
                            st.error(f"**Query Error âŒ**\n```\n{result_data['error']}\n```")
                        else:
                            st.markdown("**Query Result ğŸ“**")
                            df = pd.DataFrame(result_data.get("data", []), columns=result_data.get("columns", []))
                            st.dataframe(df, use_container_width=True)
                            if result_data.get("truncated"):
                                st.info(f"ê²°ê³¼ê°€ ë„ˆë¬´ ë§ì•„ ìµœëŒ€ {len(df)}ê±´ë§Œ í‘œì‹œí•©ë‹ˆë‹¤.")
                    except (json.JSONDecodeError, TypeError):
                        st.markdown(f"**Query Result ğŸ“**\n```\n{final_state['result']}\n```")

            return final_answer or "ë‹µë³€ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

        try:
            final_response = asyncio.run(stream_and_display())
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            final_response = "ì£„ì†¡í•©ë‹ˆë‹¤, ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."

        response_container.markdown(final_response)
        st.session_state.messages.append({"role": "assistant", "content": final_response})